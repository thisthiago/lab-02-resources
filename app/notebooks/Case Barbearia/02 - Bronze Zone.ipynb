{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44895d-59ec-48fd-8718-59009780c2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType, StringType, BinaryType, IntegerType, DoubleType, TimestampType, DateType\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from typing import Union, Optional\n",
    "\n",
    "# --- Credenciais AWS ---\n",
    "accessKeyId = \"\"\n",
    "secretAccessKey = \"\"\n",
    "\n",
    "# --- Sessão Spark ---\n",
    "def create_spark_session() -> SparkSession:\n",
    "    spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Bronze Zone\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "    conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "    conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    conf.set(\"fs.s3a.fast.upload\", \"true\")\n",
    "    conf.set(\"fs.s3a.bucket.all.committer.magic.enabled\", \"true\")\n",
    "    conf.set(\"fs.s3a.directory.marker.retention\", \"keep\")\n",
    "    conf.set(\"spark.driver.extraClassPath\", \"/usr/local/spark/jars/*\")\n",
    "    conf.set(\"spark.driver.memory\", \"8g\")\n",
    "    conf.set(\"spark.executor.memory\", \"16g\")\n",
    "    conf.set(\"fs.s3a.access.key\", accessKeyId)\n",
    "    conf.set(\"fs.s3a.secret.key\", secretAccessKey)\n",
    "\n",
    "    return spark\n",
    "\n",
    "# --- Funções auxiliares ---\n",
    "def filter_by_max_max_date(spark: SparkSession, df: DataFrame, pk_column: str, time_column: str = 'ingestion_time') -> DataFrame:\n",
    "    if time_column not in df.columns:\n",
    "        print(f\"Aviso: Coluna '{time_column}' não encontrada. Retornando DataFrame original.\")\n",
    "        return df\n",
    "\n",
    "    window_spec = Window.partitionBy(pk_column).orderBy(F.col(f\"{time_column}\").desc())\n",
    "\n",
    "    return (\n",
    "        df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "          .filter(F.col(\"row_num\") == 1)\n",
    "          .drop(\"row_num\")\n",
    "    )\n",
    "\n",
    "def get_latest_ingestion(spark: SparkSession, df: DataFrame) -> DataFrame:\n",
    "    if \"ingestion_time\" not in df.columns:\n",
    "        print(\"Aviso: Coluna 'ingestion_time' não encontrada. Retornando DataFrame vazio.\")\n",
    "        return df.limit(0)\n",
    "\n",
    "    max_ingestion_time = df.select(F.max(\"ingestion_time\")).first()[0]\n",
    "    return df.filter(F.col(\"ingestion_time\") == max_ingestion_time)\n",
    "\n",
    "def ensure_column_exists(\n",
    "    spark: SparkSession,\n",
    "    df: DataFrame,\n",
    "    column_name: str,\n",
    "    default_value: Union[str, int, float, bool, None] = None,\n",
    "    column_type: Optional[str] = None\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Verifica se uma coluna existe no DataFrame e a cria se necessário.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame do PySpark\n",
    "        column_name: Nome da coluna a ser verificada/criada\n",
    "        default_value: Valor padrão para a nova coluna (None por padrão)\n",
    "        column_type: Tipo da coluna (opcional, inferido do default_value se não especificado)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com a coluna garantida\n",
    "    \n",
    "    Exemplos:\n",
    "        >>> df = ensure_column_exists(df, \"data_criacao\", default_value=None, column_type=\"timestamp\")\n",
    "    \"\"\"\n",
    "    if column_name in df.columns:\n",
    "        return df\n",
    "    \n",
    "    print(f\"Coluna '{column_name}' não encontrada. Criando com valor padrão...\")\n",
    "    \n",
    "    # Determina o tipo da coluna se não especificado\n",
    "    if column_type is None:\n",
    "        if isinstance(default_value, bool):\n",
    "            column_type = \"boolean\"\n",
    "        elif isinstance(default_value, int):\n",
    "            column_type = \"integer\"\n",
    "        elif isinstance(default_value, float):\n",
    "            column_type = \"double\"\n",
    "        elif isinstance(default_value, str):\n",
    "            column_type = \"string\"\n",
    "        else:\n",
    "            column_type = \"string\"  # Default para outros tipos\n",
    "    \n",
    "    # Cria a coluna com o tipo apropriado\n",
    "    if default_value is None:\n",
    "        new_df = df.withColumn(column_name, F.lit(None).cast(column_type))\n",
    "    else:\n",
    "        new_df = df.withColumn(column_name, F.lit(default_value).cast(column_type))\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def delta_table_exists(spark, path):\n",
    "    \"\"\"\n",
    "    Verifica se uma tabela Delta existe no caminho especificado\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        path: Caminho para a tabela Delta (pode ser caminho S3, HDFS ou local)\n",
    "        \n",
    "    Returns:\n",
    "        bool: True se a tabela existe, False caso contrário\n",
    "    \"\"\"\n",
    "    try:\n",
    "        DeltaTable.forPath(spark, path)\n",
    "        return True\n",
    "    except AnalysisException as e:\n",
    "        if 'is not a Delta table' in str(e) or 'Path does not exist' in str(e):\n",
    "            return False\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        # Captura outros possíveis erros\n",
    "        if 'does not exist' in str(e):\n",
    "            return False\n",
    "        raise        \n",
    "\n",
    "        \n",
    "        \n",
    "def upsert_with_delete_track(spark: SparkSession, source_df: DataFrame, delta_path: str, pk_column: str, ingestion_time_column: str = \"ingestion_time\", table_name: str = None):\n",
    "    delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "    target_df = delta_table.toDF()\n",
    "    ### LOG SCHEMAS ####\n",
    "    #print(\"Schema da tabela Delta de destino:\")\n",
    "    #target_df.printSchema()\n",
    "    #target_df.show(5, truncate=False)\n",
    "\n",
    "    #print(\"Schema do DataFrame de origem:\")\n",
    "    #source_df.printSchema()\n",
    "    #source_df.show(5, truncate=False)\n",
    "    ####################\n",
    "    \n",
    "    source_df.createOrReplaceTempView(\"source_data\")\n",
    "    target_df.createOrReplaceTempView(\"target_data\")\n",
    "    \n",
    "    \n",
    "    records_to_deactivate = spark.sql(f\"\"\"\n",
    "        SELECT t.{pk_column}\n",
    "        FROM target_data t\n",
    "        LEFT JOIN source_data s ON t.{pk_column} = s.{pk_column}\n",
    "        WHERE s.{pk_column} IS NULL AND t.ativo = true\n",
    "    \"\"\")\n",
    "\n",
    "    count_to_deactivate = records_to_deactivate.count()\n",
    "    print(f\"Registros a desativar: {count_to_deactivate}\")\n",
    "    records_to_deactivate.show(truncate=False)\n",
    "    ids_to_deactivate = records_to_deactivate.select(pk_column).rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    if ids_to_deactivate:\n",
    "        print(f\"Encontrados {len(ids_to_deactivate)} registros para desativar\")\n",
    "\n",
    "        delta_table.update(\n",
    "            condition=F.col(pk_column).isin(ids_to_deactivate) & (F.col(\"ativo\") == True),\n",
    "            set={\n",
    "                \"ativo\": F.lit(False),\n",
    "                \"deletion_time\": F.current_timestamp(),\n",
    "                \"ingestion_time\": F.current_timestamp()\n",
    "            }\n",
    "        )\n",
    "        print(\"Registros desativados.\")\n",
    "    else:\n",
    "        print(\"Nenhum registro para desativar encontrado.\")\n",
    "    \n",
    "    # UPSERT\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        source_df.alias(\"source\"),\n",
    "        f\"target.{pk_column} = source.{pk_column}\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=f\"source.{ingestion_time_column} > target.{ingestion_time_column}\",\n",
    "        set={\n",
    "            col: f\"source.{col}\"\n",
    "            for col in source_df.columns\n",
    "            if col != pk_column and col in target_df.columns\n",
    "        }\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "    print(\"UPSERT concluído!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ca76b68-e8fb-4389-83a0-2feb41511c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.05 ms, sys: 1.49 ms, total: 3.53 ms\n",
      "Wall time: 4.86 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark = create_spark_session()\n",
    "\n",
    "# --- Paths ---\n",
    "landing = \"s3a://dev-lab-02-us-east-2-landing/db_barbearia/\"\n",
    "bronze = \"s3a://dev-lab-02-us-east-2-bronze/db_barbearia/\"\n",
    "\n",
    "# --- Tabelas configuradas ---\n",
    "tables_configs = {\n",
    "    \"cliente\": {\"pk\": \"cliente_id\"},\n",
    "    \"profissional\": {\"pk\": \"profissional_id\"},\n",
    "    \"servico\": {\"pk\": \"servico_id\"},\n",
    "    \"agendamento\": {\"pk\": \"agendamento_id\"},\n",
    "    \"pagamento\": {\"pk\": \"pagamento_id\"},\n",
    "    \"horario_profissional\": {\"pk\": \"horario_id\"},\n",
    "    \"promocao\": {\"pk\": \"promocao_id\"},\n",
    "    \"servico_promocao\": {\"pk\": \"servico_promocao_id\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f4517b-7697-497a-83a3-ecf9e7820d08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 8 - Inicando ingestão na bronze para a tabela de cliente\n",
      "Registros a desativar: 0\n",
      "+----------+\n",
      "|cliente_id|\n",
      "+----------+\n",
      "+----------+\n",
      "\n",
      "Nenhum registro para desativar encontrado.\n",
      "UPSERT concluído.\n",
      "2 / 8 - Inicando ingestão na bronze para a tabela de profissional\n",
      "Registros a desativar: 0\n",
      "+---------------+\n",
      "|profissional_id|\n",
      "+---------------+\n",
      "+---------------+\n",
      "\n",
      "Nenhum registro para desativar encontrado.\n",
      "UPSERT concluído.\n",
      "3 / 8 - Inicando ingestão na bronze para a tabela de servico\n",
      "Registros a desativar: 0\n",
      "+----------+\n",
      "|servico_id|\n",
      "+----------+\n",
      "+----------+\n",
      "\n",
      "Nenhum registro para desativar encontrado.\n",
      "UPSERT concluído.\n",
      "4 / 8 - Inicando ingestão na bronze para a tabela de agendamento\n",
      "Coluna 'ativo' não encontrada. Criando com valor padrão...\n",
      "Registros a desativar: 0\n",
      "+--------------+\n",
      "|agendamento_id|\n",
      "+--------------+\n",
      "+--------------+\n",
      "\n",
      "Nenhum registro para desativar encontrado.\n",
      "UPSERT concluído.\n",
      "5 / 8 - Inicando ingestão na bronze para a tabela de pagamento\n",
      "Coluna 'ativo' não encontrada. Criando com valor padrão...\n",
      "Registros a desativar: 0\n",
      "+------------+\n",
      "|pagamento_id|\n",
      "+------------+\n",
      "+------------+\n",
      "\n",
      "Nenhum registro para desativar encontrado.\n",
      "UPSERT concluído.\n",
      "6 / 8 - Inicando ingestão na bronze para a tabela de horario_profissional\n",
      "Registros a desativar: 0\n",
      "+----------+\n",
      "|horario_id|\n",
      "+----------+\n",
      "+----------+\n",
      "\n",
      "Nenhum registro para desativar encontrado.\n",
      "UPSERT concluído.\n",
      "7 / 8 - Inicando ingestão na bronze para a tabela de promocao\n",
      "Registros a desativar: 0\n",
      "+-----------+\n",
      "|promocao_id|\n",
      "+-----------+\n",
      "+-----------+\n",
      "\n",
      "Nenhum registro para desativar encontrado.\n",
      "UPSERT concluído.\n",
      "8 / 8 - Inicando ingestão na bronze para a tabela de servico_promocao\n",
      "Coluna 'ativo' não encontrada. Criando com valor padrão...\n",
      "Registros a desativar: 0\n",
      "+-------------------+\n",
      "|servico_promocao_id|\n",
      "+-------------------+\n",
      "+-------------------+\n",
      "\n",
      "Nenhum registro para desativar encontrado.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "contador = 0\n",
    "for table_name, table_config in tables_configs.items():\n",
    "    contador+=1\n",
    "    print(f\"{contador} / {len(tables_configs)} - Inicando ingestão na bronze para a tabela de {table_name}\")\n",
    "    \n",
    "    # --- Pre-processamento ---\n",
    "    df_landing = spark.read.format(\"parquet\").load(f\"{landing}{table_name}/\")\n",
    "    df_landing = ensure_column_exists(spark, df_landing, \"ativo\", default_value=True, column_type=\"boolean\")\n",
    "    df_landing = df_landing.withColumn(\"deletion_time\", F.lit(None).cast(TimestampType()))\n",
    "    df_landing = filter_by_max_max_date(spark, df_landing, table_config['pk'])\n",
    "    df_landing = get_latest_ingestion(spark, df_landing)\n",
    "    \n",
    "    \n",
    "    # --- Ingestão ---\n",
    "    if delta_table_exists(spark, f\"{bronze}{table_name}/\"):\n",
    "        upsert_with_delete_track(\n",
    "            spark,\n",
    "            source_df=df_landing,\n",
    "            delta_path=f\"{bronze}{table_name}/\",\n",
    "            pk_column=table_config['pk'],\n",
    "            ingestion_time_column=\"ingestion_time\",\n",
    "            table_name=table_name\n",
    "        )\n",
    "    else:\n",
    "        df_landing.write.format(\"delta\").save(f\"{bronze}{table_name}/\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c8529eb-8c68-4d28-98c3-a8c3b5f47647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(\"s3a://dev-lab-02-us-east-2-bronze/db_barbearia/cliente/\").createOrReplaceTempView(\"cliente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3005e2f-0c91-4f47-a73c-654718e60b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     501|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from cliente\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ee91640-5eae-4a3f-b85a-724feddc56d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|nome              |ativo|\n",
      "+------------------+-----+\n",
      "|Dr. Gabriel Pastor|false|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Original\n",
    "spark.sql(\"select nome,ativo from cliente where cliente_id in (1008,2)\").show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f70e45fe-7bd4-415d-842c-2b95b253f32f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+-------------+\n",
      "|nome                 |ativo|deletion_time|\n",
      "+---------------------+-----+-------------+\n",
      "|Amaurir              |true |null         |\n",
      "|Dr. Gabriel Pastor Dr|false|null         |\n",
      "+---------------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#insert\n",
    "spark.sql(\"select nome,ativo,deletion_time from cliente where cliente_id in (1008,2)\").show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4cb5ce3-b504-4d12-902b-114dc07b9c69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------------------------+\n",
      "|nome       |ativo|deletion_time             |\n",
      "+-----------+-----+--------------------------+\n",
      "|Amaurir    |false|2025-05-04 22:39:26.023927|\n",
      "|Dr. Gabriel|false|null                      |\n",
      "+-----------+-----+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#delete\n",
    "spark.sql(\"select nome,ativo,deletion_time from cliente where cliente_id in (1008,2)\").show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca1ee25e-ac64-4504-8bfa-e33f73bc14a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 1 - Inicando ingestão na bronze para a tabela de servico_promocao\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
