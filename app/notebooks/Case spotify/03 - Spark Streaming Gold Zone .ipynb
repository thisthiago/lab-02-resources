{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee694ac-5c61-48bb-8e08-5a72a1f4bf92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BinaryType, IntegerType, DoubleType, TimestampType, DateType, LongType\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from typing import Union, Optional\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "# --- Credenciais AWS ---\n",
    "accessKeyId = \"\"\n",
    "secretAccessKey = \"\"\n",
    "\n",
    "# --- SessÃ£o Spark ---\n",
    "def create_spark_session() -> SparkSession:\n",
    "    spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Gold Zone Streaming\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "    conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "    conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    conf.set(\"fs.s3a.fast.upload\", \"true\")\n",
    "    conf.set(\"fs.s3a.bucket.all.committer.magic.enabled\", \"true\")\n",
    "    conf.set(\"fs.s3a.directory.marker.retention\", \"keep\")\n",
    "    conf.set(\"spark.driver.extraClassPath\", \"/usr/local/spark/jars/*\")\n",
    "    conf.set(\"spark.driver.memory\", \"8g\")\n",
    "    conf.set(\"spark.executor.memory\", \"16g\")\n",
    "    conf.set(\"fs.s3a.access.key\", accessKeyId)\n",
    "    conf.set(\"fs.s3a.secret.key\", secretAccessKey)\n",
    "\n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a066d5a2-b738-4ab0-b5da-b35ee99fa678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "silver_path = \"s3a://dev-lab-02-us-east-2-silver/fato_streamings\"\n",
    "gold_path = f\"s3a://dev-lab-02-us-east-2-gold/top_artistas\"\n",
    "checkpoint_path = f\"s3a://dev-lab-02-us-east-2-gold/checkpoint/fato_streamings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bc1246d-cd18-40e7-ae65-3d7f3da24132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_fato_streamings = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"ignoreChanges\", \"true\") \\\n",
    "    .load(f\"{silver_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f36dde1d-583e-4ac6-a232-b50b684c729b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_filtrado = df_fato_streamings.filter(df_fato_streamings.flg_feat == False)\n",
    "\n",
    "\n",
    "df_artistas_top = df_filtrado.groupBy(\"artista\") \\\n",
    "    .agg(F.count(\"*\").alias(\"qtd_de_streaming\")) \\\n",
    "    .orderBy(F.col(\"qtd_de_streaming\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "df_artistas_top_com_data = df_artistas_top.withColumn(\"data_atualizacao\", F.current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e74614-a6b2-4761-b889-afbe61273c6f",
   "metadata": {},
   "source": [
    "Com o outputMode(\"complete\"), o Spark vai recalcular toda a agregaÃ§Ã£o e sobrescrever os dados da saÃ­da (Gold Path) a cada vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdb8f0e1-c138-4513-aafc-b78d4733e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.trigger(processingTime=\"1 minute\") \\\n",
    "#.trigger(once=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1759b2e-7f58-4564-a6e0-b8a6e5a1788e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query  = df_artistas_top_com_data.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}\") \\\n",
    "    .option(\"path\", f\"{gold_path}\") \\\n",
    "    .trigger(processingTime=\"1 minute\") \\\n",
    "    .start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9fadc1-0f3b-4c22-8cd3-c781c9fd437e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modos do trigger() no Spark Streaming\n",
    "\n",
    "No Spark Streaming, o parÃ¢metro 'trigger()' define a frequÃªncia com que o micro-batch (janela de dados) Ã© processado.\n",
    "Existem diferentes tipos de gatilhos (triggers) que controlam a execuÃ§Ã£o do processamento.\n",
    "\n",
    "#### 1. 'ProcessingTime' â±ï¸\n",
    "- O trigger baseado no tempo de processamento define a frequÃªncia fixa com que os batches sÃ£o executados.\n",
    "- VocÃª pode especificar o intervalo de tempo (em milissegundos) para o Spark processar os dados em intervalos regulares.\n",
    "- Ã‰ o modo padrÃ£o, com o valor de 500 milissegundos\n",
    "Exemplo:\n",
    "\n",
    "O Spark irÃ¡ processar os dados a cada 10 segundos, por exemplo.\n",
    "```python\n",
    "processing_time_example = \"\"\"\n",
    "# Exemplo de cÃ³digo para 'ProcessingTime' â±ï¸\n",
    "streaming_data \\\n",
    "  .writeStream \\\n",
    "  .trigger(processingTime=\"10 seconds\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "  .start(\"/path/to/output\")\n",
    "\"\"\"\n",
    "```\n",
    "#### 2. 'Once' ðŸ”„â²ï¸\n",
    "- O trigger 'once' farÃ¡ o Spark processar todos os dados disponÃ­veis no stream uma Ãºnica vez.\n",
    "- ApÃ³s processar os dados, o streaming serÃ¡ encerrado.\n",
    "- Ideal para processamentos em lote em vez de streaming contÃ­nuo.\n",
    "\n",
    "Exemplo:\n",
    "O Spark irÃ¡ processar os dados uma vez e, depois disso, o stream Ã© interrompido.\n",
    "```python\n",
    "once_example = \"\"\"\n",
    "# Exemplo de cÃ³digo para 'Once' ðŸ”„â²ï¸\n",
    "streaming_data \\\n",
    "  .writeStream \\\n",
    "  .trigger(once=True) \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "  .start(\"/path/to/output\")\n",
    "\"\"\"\n",
    "```\n",
    "#### 3. 'Continuous' ðŸ”„âš¡\n",
    "- O trigger 'continuous' permite o processamento de dados de forma contÃ­nua com o menor intervalo de tempo possÃ­vel.\n",
    "- Ideal para processamento em tempo real com latÃªncia muito baixa.\n",
    "- Requer o modo de execuÃ§Ã£o 'continuous processing' e oferece uma execuÃ§Ã£o quase constante.\n",
    "\n",
    "Exemplo:\n",
    "O Spark irÃ¡ processar os dados assim que eles chegarem, com o mÃ­nimo de latÃªncia.\n",
    "```python\n",
    "continuous_example = \"\"\"\n",
    "Exemplo de cÃ³digo para 'Continuous' ðŸ”„âš¡\n",
    "streaming_data \\\n",
    "  .writeStream \\\n",
    "  .trigger(continuous=\"1 second\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "  .start(\"/path/to/output\")\n",
    "\"\"\"\n",
    "```\n",
    "#### Resumo:\n",
    "- **ProcessingTime** â±ï¸: processa os dados em intervalos fixos de tempo.\n",
    "- **Once** ðŸ”„â²ï¸: processa os dados uma Ãºnica vez e interrompe o streaming.\n",
    "- **Continuous** ðŸ”„âš¡: processa os dados de forma contÃ­nua com latÃªncia mÃ­nima.\n",
    "\n",
    "> A escolha do trigger depende das necessidades de processamento em tempo real e do volume de dados.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
