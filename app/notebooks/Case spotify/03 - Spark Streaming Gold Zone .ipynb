{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee694ac-5c61-48bb-8e08-5a72a1f4bf92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BinaryType, IntegerType, DoubleType, TimestampType, DateType, LongType\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from typing import Union, Optional\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "# --- Credenciais AWS ---\n",
    "accessKeyId = \"\"\n",
    "secretAccessKey = \"\"\n",
    "\n",
    "# --- Sessão Spark ---\n",
    "def create_spark_session() -> SparkSession:\n",
    "    spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Gold Zone Streaming\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "    conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "    conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    conf.set(\"fs.s3a.fast.upload\", \"true\")\n",
    "    conf.set(\"fs.s3a.bucket.all.committer.magic.enabled\", \"true\")\n",
    "    conf.set(\"fs.s3a.directory.marker.retention\", \"keep\")\n",
    "    conf.set(\"spark.driver.extraClassPath\", \"/usr/local/spark/jars/*\")\n",
    "    conf.set(\"spark.driver.memory\", \"8g\")\n",
    "    conf.set(\"spark.executor.memory\", \"16g\")\n",
    "    conf.set(\"fs.s3a.access.key\", accessKeyId)\n",
    "    conf.set(\"fs.s3a.secret.key\", secretAccessKey)\n",
    "\n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a066d5a2-b738-4ab0-b5da-b35ee99fa678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "silver_path = \"s3a://dev-lab-02-us-east-2-silver/fato_streamings\"\n",
    "gold_path = f\"s3a://dev-lab-02-us-east-2-gold/top_artistas\"\n",
    "checkpoint_path = f\"s3a://dev-lab-02-us-east-2-gold/checkpoint/fato_streamings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bc1246d-cd18-40e7-ae65-3d7f3da24132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_fato_streamings = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"ignoreChanges\", \"true\") \\\n",
    "    .load(f\"{silver_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f36dde1d-583e-4ac6-a232-b50b684c729b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_filtrado = df_fato_streamings.filter(df_fato_streamings.flg_feat == False)\n",
    "\n",
    "\n",
    "df_artistas_top = df_filtrado.groupBy(\"artista\") \\\n",
    "    .agg(F.count(\"*\").alias(\"qtd_de_streaming\")) \\\n",
    "    .orderBy(F.col(\"qtd_de_streaming\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "df_artistas_top_com_data = df_artistas_top.withColumn(\"data_atualizacao\", F.current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e74614-a6b2-4761-b889-afbe61273c6f",
   "metadata": {},
   "source": [
    "Com o outputMode(\"complete\"), o Spark vai recalcular toda a agregação e sobrescrever os dados da saída (Gold Path) a cada vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdb8f0e1-c138-4513-aafc-b78d4733e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.trigger(processingTime=\"1 minute\") \\\n",
    "#.trigger(once=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1759b2e-7f58-4564-a6e0-b8a6e5a1788e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query  = df_artistas_top_com_data.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}\") \\\n",
    "    .option(\"path\", f\"{gold_path}\") \\\n",
    "    .trigger(processingTime=\"1 minute\") \\\n",
    "    .start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9fadc1-0f3b-4c22-8cd3-c781c9fd437e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modos do trigger() no Spark Streaming\n",
    "\n",
    "No Spark Streaming, o parâmetro 'trigger()' define a frequência com que o micro-batch (janela de dados) é processado.\n",
    "Existem diferentes tipos de gatilhos (triggers) que controlam a execução do processamento.\n",
    "\n",
    "#### 1. 'ProcessingTime' ⏱️\n",
    "- O trigger baseado no tempo de processamento define a frequência fixa com que os batches são executados.\n",
    "- Você pode especificar o intervalo de tempo (em milissegundos) para o Spark processar os dados em intervalos regulares.\n",
    "- É o modo padrão, com o valor de 500 milissegundos\n",
    "Exemplo:\n",
    "\n",
    "O Spark irá processar os dados a cada 10 segundos, por exemplo.\n",
    "```python\n",
    "processing_time_example = \"\"\"\n",
    "# Exemplo de código para 'ProcessingTime' ⏱️\n",
    "streaming_data \\\n",
    "  .writeStream \\\n",
    "  .trigger(processingTime=\"10 seconds\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "  .start(\"/path/to/output\")\n",
    "\"\"\"\n",
    "```\n",
    "#### 2. 'Once' 🔄⏲️\n",
    "- O trigger 'once' fará o Spark processar todos os dados disponíveis no stream uma única vez.\n",
    "- Após processar os dados, o streaming será encerrado.\n",
    "- Ideal para processamentos em lote em vez de streaming contínuo.\n",
    "\n",
    "Exemplo:\n",
    "O Spark irá processar os dados uma vez e, depois disso, o stream é interrompido.\n",
    "```python\n",
    "once_example = \"\"\"\n",
    "# Exemplo de código para 'Once' 🔄⏲️\n",
    "streaming_data \\\n",
    "  .writeStream \\\n",
    "  .trigger(once=True) \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "  .start(\"/path/to/output\")\n",
    "\"\"\"\n",
    "```\n",
    "#### 3. 'Continuous' 🔄⚡\n",
    "- O trigger 'continuous' permite o processamento de dados de forma contínua com o menor intervalo de tempo possível.\n",
    "- Ideal para processamento em tempo real com latência muito baixa.\n",
    "- Requer o modo de execução 'continuous processing' e oferece uma execução quase constante.\n",
    "\n",
    "Exemplo:\n",
    "O Spark irá processar os dados assim que eles chegarem, com o mínimo de latência.\n",
    "```python\n",
    "continuous_example = \"\"\"\n",
    "Exemplo de código para 'Continuous' 🔄⚡\n",
    "streaming_data \\\n",
    "  .writeStream \\\n",
    "  .trigger(continuous=\"1 second\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "  .start(\"/path/to/output\")\n",
    "\"\"\"\n",
    "```\n",
    "#### Resumo:\n",
    "- **ProcessingTime** ⏱️: processa os dados em intervalos fixos de tempo.\n",
    "- **Once** 🔄⏲️: processa os dados uma única vez e interrompe o streaming.\n",
    "- **Continuous** 🔄⚡: processa os dados de forma contínua com latência mínima.\n",
    "\n",
    "> A escolha do trigger depende das necessidades de processamento em tempo real e do volume de dados.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
