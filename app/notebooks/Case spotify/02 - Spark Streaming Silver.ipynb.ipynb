{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a945d-6df2-4153-aaa3-eacd35af8b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, expr, split\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BinaryType, IntegerType, DoubleType, TimestampType, DateType, LongType\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from typing import Union, Optional\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "# --- Credenciais AWS ---\n",
    "accessKeyId = \"\"\n",
    "secretAccessKey = \"\"\n",
    "\n",
    "# --- Sess√£o Spark ---\n",
    "def create_spark_session() -> SparkSession:\n",
    "    spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Silver Zone Streaming\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "    conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "    conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    conf.set(\"fs.s3a.fast.upload\", \"true\")\n",
    "    conf.set(\"fs.s3a.bucket.all.committer.magic.enabled\", \"true\")\n",
    "    conf.set(\"fs.s3a.directory.marker.retention\", \"keep\")\n",
    "    conf.set(\"spark.driver.extraClassPath\", \"/usr/local/spark/jars/*\")\n",
    "    conf.set(\"spark.driver.memory\", \"8g\")\n",
    "    conf.set(\"spark.executor.memory\", \"16g\")\n",
    "    conf.set(\"fs.s3a.access.key\", accessKeyId)\n",
    "    conf.set(\"fs.s3a.secret.key\", secretAccessKey)\n",
    "\n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d406828-8c65-4566-b417-766d50f98f10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "bronze_path = \"s3a://dev-lab-02-us-east-2-bronze/spotify/\"\n",
    "silver_path = \"s3a://dev-lab-02-us-east-2-silver/\"\n",
    "silver_table = \"fato_streamings\"\n",
    "silver_table_path = f\"{silver_path}{silver_table}\"\n",
    "checkpoint_path = f\"{silver_path}/checkpoints/{silver_table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea80b816-3146-4308-ac8a-8f491c2ad352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "usuarios_df = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"ignoreChanges\", \"true\") \\\n",
    "    .load(f\"{bronze_path}usuarios\")\n",
    "\n",
    "streamings_df = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"ignoreChanges\", \"true\") \\\n",
    "    .load(f\"{bronze_path}streamings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f755b180-bf37-4609-ae68-056a3aa9759a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_result = (\n",
    "    streamings_df.alias(\"s\")\n",
    "    .join(usuarios_df.alias(\"u\"), col(\"s.nome\") == col(\"u.nome\"), \"inner\")\n",
    "    .withColumn(\"masked_email\", expr(\"\"\"\n",
    "        CONCAT(\n",
    "            SUBSTRING(u.email, 1, 1),\n",
    "            REPEAT('*', INSTR(u.email, '@') - 2),\n",
    "            SUBSTRING(u.email, INSTR(u.email, '@'), LENGTH(u.email))\n",
    "        )\n",
    "    \"\"\"))\n",
    "    .withColumn(\"musica_bruta\", col(\"s.musica\"))\n",
    "    .withColumn(\"artista\", split(col(\"musica_bruta\"), \"-\")[0])\n",
    "    .withColumn(\"musica\", split(col(\"musica_bruta\"), \"-\")[1])\n",
    "    .withColumn(\"flg_feat\", expr(\"INSTR(split(musica_bruta, '-')[1], 'w/') > 0\"))\n",
    "    .select(\n",
    "        col(\"s.id\"),\n",
    "        col(\"u.id\").alias(\"id_usuario\"),\n",
    "        col(\"masked_email\"),\n",
    "        col(\"artista\"),\n",
    "        col(\"musica\"),\n",
    "        col(\"flg_feat\"),\n",
    "        col(\"s.timestamp\"),\n",
    "        col(\"s.origem_arquivo\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1d057ed-d22c-4bc8-b248-309dc5f169b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upsert_to_delta(microbatch_df, batch_id):\n",
    "    if microbatch_df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    windowed_df = (\n",
    "        microbatch_df\n",
    "        .withColumn(\"timestamp\", F.col(\"timestamp\").cast(\"timestamp\"))\n",
    "        .dropDuplicates([\"id\"])  # garante que n√£o haja duplicatas por ID no microbatch\n",
    "    )\n",
    "    \n",
    "    if DeltaTable.isDeltaTable(spark, silver_table_path):\n",
    "        delta_table = DeltaTable.forPath(spark, silver_table_path)\n",
    "\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            windowed_df.alias(\"source\"),\n",
    "            \"target.id = source.id\"\n",
    "        ).whenMatchedUpdateAll(\n",
    "        ).whenNotMatchedInsertAll(\n",
    "        ).execute()\n",
    "\n",
    "    else:\n",
    "        windowed_df.write.format(\"delta\").mode(\"overwrite\").save(silver_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ec9a0-ace5-4e12-9ea0-04e2cde1d65f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    df_result.writeStream\n",
    "    .foreachBatch(upsert_to_delta)\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"1 minute\")\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be37dfd-2944-4dfc-b5b1-2f81a4aa3f6f",
   "metadata": {},
   "source": [
    "## Modos de sa√≠da do Spark Streaming (outputMode)\n",
    "\n",
    "No Spark Streaming, o par√¢metro 'outputMode' define como os dados processados nas janelas de tempo (batches)\n",
    "devem ser gravados na sa√≠da, seja para um arquivo, banco de dados ou outro destino.\n",
    "Existem tr√™s modos principais:\n",
    "\n",
    "#### 1. 'append' ‚ûï\n",
    " - Apenas os novos dados (que chegaram no batch atual) s√£o adicionados √† sa√≠da.\n",
    " - Ideal para cen√°rios onde voc√™ s√≥ precisa de registros novos, sem reprocessar os antigos.\n",
    " - √â o modo padr√£o \n",
    " \n",
    "Exemplo:\n",
    "\n",
    "Em um stream de vendas, voc√™ apenas adiciona novas vendas ao seu arquivo ou banco de dados, \n",
    "sem alterar os registros anteriores.\n",
    "\n",
    "```python\n",
    "append_example = \"\"\"\n",
    "# Exemplo de c√≥digo para 'append' ‚ûï\n",
    "streaming_data \\\n",
    "  .writeStream \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "  .start(\"/path/to/output\")\n",
    "\"\"\"\n",
    "\n",
    "```\n",
    "#### 2. 'complete' üîÑ\n",
    "- Todos os dados processados at√© o momento s√£o recalculados e escritos na sa√≠da.\n",
    "- Ideal para cen√°rios onde voc√™ precisa de um estado completo de todos os dados processados at√© aquele ponto,\n",
    "  como somar todos os valores ou gerar uma vis√£o agregada.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "Em um stream de contagem de palavras, voc√™ recalcula e escreve a contagem total de todas as palavras \n",
    "desde o in√≠cio, n√£o apenas as novas.\n",
    "```python\n",
    "complete_example = \"\"\"\n",
    "# Exemplo de c√≥digo para 'complete' üîÑ\n",
    "streaming_data \\\n",
    "  .groupBy(\"word\") \\\n",
    "  .count() \\\n",
    "  .writeStream \\\n",
    "  .outputMode(\"complete\") \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "  .start(\"/path/to/output\")\n",
    "\"\"\"\n",
    "```\n",
    "#### 3. 'update' üîÑ‚úèÔ∏è\n",
    "- Apenas os dados que foram atualizados (ou mudaram) desde a √∫ltima vez que foram processados s√£o gravados na sa√≠da.\n",
    "- Ideal para cen√°rios onde voc√™ n√£o precisa recalcular tudo, mas deseja registrar as atualiza√ß√µes.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "Em um stream de contagem de palavras, voc√™ apenas atualiza a contagem das palavras que mudaram, \n",
    "sem recalcular todas as contagens.\n",
    "\n",
    "```python\n",
    "update_example = \"\"\"\n",
    "# Exemplo de c√≥digo para 'update' üîÑ‚úèÔ∏è\n",
    "streaming_data \\\n",
    "  .groupBy(\"word\") \\\n",
    "  .count() \\\n",
    "  .writeStream \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "  .start(\"/path/to/output\")\n",
    "\"\"\"\n",
    "```\n",
    "#### Resumo:\n",
    "- **append** ‚ûï: apenas os novos dados s√£o gravados.\n",
    "- **complete** üîÑ: todos os dados s√£o recalculados e gravados.\n",
    "- **update** üîÑ‚úèÔ∏è: apenas as atualiza√ß√µes dos dados s√£o gravadas.\n",
    "\n",
    "> Escolher o 'outputMode' adequado depende do tipo de processamento e dos requisitos da aplica√ß√£o.\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
