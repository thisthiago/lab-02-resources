{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a744215-fc87-45c1-92ff-d417bfc48770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BinaryType, IntegerType, DoubleType, TimestampType, DateType, LongType\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from typing import Union, Optional\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# --- Credenciais AWS ---\n",
    "accessKeyId = \"\"\n",
    "secretAccessKey = \"\"\n",
    "\n",
    "# --- Sess√£o Spark ---\n",
    "def create_spark_session() -> SparkSession:\n",
    "    spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Bronze Zone Streaming\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "    conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "    conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    conf.set(\"fs.s3a.fast.upload\", \"true\")\n",
    "    conf.set(\"fs.s3a.bucket.all.committer.magic.enabled\", \"true\")\n",
    "    conf.set(\"fs.s3a.directory.marker.retention\", \"keep\")\n",
    "    conf.set(\"spark.driver.extraClassPath\", \"/usr/local/spark/jars/*\")\n",
    "    conf.set(\"spark.driver.memory\", \"8g\")\n",
    "    conf.set(\"spark.executor.memory\", \"16g\")\n",
    "    conf.set(\"fs.s3a.access.key\", accessKeyId)\n",
    "    conf.set(\"fs.s3a.secret.key\", secretAccessKey)\n",
    "\n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c4977b-ee2b-4bb1-94c1-d74dad3968ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema_usuarios = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema_musicas = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"Artist\", StringType(), True),\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema_streamings = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"musica\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea73739b-b11e-4698-9ab5-4643ea3c7158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "landing_path = f\"s3a://dev-lab-02-us-east-2-landing/spotify/\"\n",
    "bronze_path = f\"s3a://dev-lab-02-us-east-2-bronze/spotify/\"\n",
    "checkpoint_path = f\"s3a://dev-lab-02-us-east-2-bronze/checkpoints/spotify/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371fc656-81a3-482f-ae8c-83703f65aad6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Explica√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a28f0a-ff73-400f-b8ec-f751e0da30dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `readStream` ‚Äî Leitura de Dados em \"Tempo Real\" no Spark\n",
    "\n",
    "### üìå O que √©?\n",
    "\n",
    "`readStream` √© o m√©todo do PySpark para **ler dados em tempo real (streaming)**. Ele permite que sua aplica√ß√£o Spark reaja automaticamente a **novos arquivos ou mensagens** que chegam a um diret√≥rio, Kafka, socket, entre outros.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Quando usar?\n",
    "\n",
    "* Quando sua aplica√ß√£o precisa **processar dados continuamente** conforme eles chegam.\n",
    "* Ideal para ingest√£o de dados para **pipelines de streaming**: landing ‚Üí bronze ‚Üí silver.\n",
    "* Casos comuns:\n",
    "\n",
    "  * Novos arquivos JSON chegando em uma pasta no S3.\n",
    "  * Mensagens de um t√≥pico Kafka.\n",
    "  * Leitura cont√≠nua de logs, sensores ou eventos.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Exemplo b√°sico com arquivos JSON\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "df_stream = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .load(\"s3a://meu-bucket/landing/usuarios/\") \\\n",
    "    .withColumn(\"origem_arquivo\", input_file_name())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Par√¢metros √∫teis para `readStream`\n",
    "\n",
    "| Par√¢metro                     | Descri√ß√£o                                                                    |\n",
    "| ----------------------------- | ---------------------------------------------------------------------------- |\n",
    "| `.format(\"...\")`              | Fonte de dados: `json`, `csv`, `parquet`, `kafka`, `socket`, etc.            |\n",
    "| `.schema(schema)`             | Define o schema dos dados esperados (obrigat√≥rio para arquivos estruturados) |\n",
    "| `.option(\"multiline\", \"...\")` | Se for `json` ou `csv`, define se os objetos est√£o em m√∫ltiplas linhas       |\n",
    "| `.load(path)`                 | Caminho onde os dados est√£o chegando continuamente                           |\n",
    "| `.withColumn(...)`            | Pode ser usado para adicionar colunas como `origem_arquivo`, `data`, etc.    |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Aten√ß√£o ao uso com arquivos\n",
    "\n",
    "* O Spark **n√£o reprocessa arquivos antigos** por padr√£o. Ele considera apenas **novos arquivos**.\n",
    "* O diret√≥rio precisa ser **imut√°vel**: evite sobrescrever arquivos no mesmo caminho.\n",
    "* O schema precisa ser definido, pois o Spark n√£o infere schema dinamicamente em streaming.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Diferen√ßa entre `read` vs `readStream`\n",
    "\n",
    "| Caracter√≠stica        | `read` (batch)          | `readStream` (streaming)                       |\n",
    "| --------------------- | ----------------------- | ---------------------------------------------- |\n",
    "| Tipo de leitura       | Dados fixos (est√°ticos) | Dados em tempo real (din√¢micos)                |\n",
    "| Quando √© executado?   | Apenas uma vez          | Continuamente, enquanto o stream estiver ativo |\n",
    "| Suporte a formatos    | Todos                   | Limitado: JSON, CSV, Parquet, Delta, Kafka     |\n",
    "| Necessita checkpoint? | ‚ùå N√£o                   | ‚úÖ Sim, para toler√¢ncia a falhas                |\n",
    "| Schema din√¢mico       | ‚úÖ Sim                   | ‚ùå N√£o ‚Äî precisa ser definido                   |\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Curiosidade: Spark Structured Streaming √© *micro-batch*\n",
    "\n",
    "Apesar de parecer tempo real, o Spark Structured Streaming opera internamente em **micro-batches**, ou seja, ele agrupa os dados em pequenos lotes com base no tempo (por padr√£o, a cada 500ms). Esse modelo equilibra performance e toler√¢ncia a falhas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd70787-6626-44ec-9b87-c032ba06b9f9",
   "metadata": {},
   "source": [
    "## `writeStream` com `.format(\"json\" | \"delta\")`\n",
    "\n",
    "### üìå O que √©?\n",
    "\n",
    "O m√©todo `.writeStream` em PySpark √© usado para gravar os dados de um **DataFrame em tempo real** em um destino como arquivos JSON, Delta, Kafka, etc.\n",
    "\n",
    "### ‚úÖ Quando usar?\n",
    "\n",
    "* Quando voc√™ deseja **gravar continuamente** dados sem transforma√ß√£o complexa por micro-lote.\n",
    "* Ideal para modos de sa√≠da simples como `append`, `complete` ou `update`.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Exemplo b√°sico\n",
    "\n",
    "```python\n",
    "query = df_stream.writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", \"s3a://meu-bucket/saida/\") \\\n",
    "    .option(\"checkpointLocation\", \"s3a://meu-bucket/checkpoint/\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Par√¢metros √∫teis para `.writeStream`\n",
    "\n",
    "| Par√¢metro                            | Descri√ß√£o                                                                    |\n",
    "| ------------------------------------ | ---------------------------------------------------------------------------- |\n",
    "| `.format(\"...\")`                     | Define o formato de sa√≠da: `json`, `parquet`, `delta`, `console`, etc.       |\n",
    "| `.option(\"path\", ...)`               | Caminho onde os dados ser√£o salvos                                           |\n",
    "| `.option(\"checkpointLocation\", ...)` | Local para armazenar estado e falhas                                         |\n",
    "| `.outputMode(\"...\")`                 | Define o modo de sa√≠da: `append`, `complete`, `update`                       |\n",
    "| `.trigger(...)`                      | Define o intervalo de execu√ß√£o (ex: `.trigger(processingTime=\"10 seconds\")`) |\n",
    "| `.start()`                           | Inicia o stream                                                              |\n",
    "| `.awaitTermination()`                | Mant√©m o processo em execu√ß√£o                                                |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Limita√ß√µes\n",
    "\n",
    "* N√£o permite **l√≥gica personalizada por batch** (ex: upserts).\n",
    "* O modo `append` s√≥ adiciona dados novos ‚Äî n√£o faz merge nem update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d9381c-78bf-45b9-a9b1-f19f97cb20af",
   "metadata": {},
   "source": [
    "## üìù README 2: `writeStream` com `.foreachBatch(...)`\n",
    "\n",
    "### üìå O que √©?\n",
    "\n",
    "O m√©todo `.foreachBatch` permite **executar c√≥digo customizado por micro-lote**. Isso √© ideal para aplicar **transforma√ß√µes, joins, merges (upsert), valida√ß√µes**, etc.\n",
    "\n",
    "### ‚úÖ Quando usar?\n",
    "\n",
    "* Quando voc√™ precisa de **l√≥gica avan√ßada** como:\n",
    "\n",
    "  * Escrita em Delta com `MERGE` (upsert).\n",
    "  * Enriquecimento de dados.\n",
    "  * Grava√ß√£o condicional ou m√∫ltiplos destinos.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Exemplo b√°sico com UPSERT em Delta\n",
    "\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def upsert_to_delta(micro_batch_df, batch_id):\n",
    "    delta_table = DeltaTable.forPath(spark, \"s3a://bucket/saida/\")\n",
    "    delta_table.alias(\"t\").merge(\n",
    "        micro_batch_df.alias(\"s\"),\n",
    "        \"t.id = s.id\"\n",
    "    ).whenMatchedUpdateAll() \\\n",
    "     .whenNotMatchedInsertAll() \\\n",
    "     .execute()\n",
    "\n",
    "query = df_stream.writeStream \\\n",
    "    .foreachBatch(upsert_to_delta) \\\n",
    "    .option(\"checkpointLocation\", \"s3a://bucket/checkpoint/\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Par√¢metros √∫teis para `.foreachBatch`\n",
    "\n",
    "| Par√¢metro                            | Descri√ß√£o                                         |\n",
    "| ------------------------------------ | ------------------------------------------------- |\n",
    "| `.foreachBatch(func)`                | Fun√ß√£o que recebe `(df, batch_id)` por micro-lote |\n",
    "| `.option(\"checkpointLocation\", ...)` | Local onde o Spark salva o estado do stream       |\n",
    "| `.trigger(...)`                      | Define o intervalo de micro-batches               |\n",
    "| `.start()`                           | Inicia o stream                                   |\n",
    "| `.awaitTermination()`                | Mant√©m a aplica√ß√£o ativa                          |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Vantagens\n",
    "\n",
    "* Permite **grava√ß√£o com l√≥gica condicional** (ex: MERGE).\n",
    "* Pode escrever em qualquer destino: Delta, JDBC, MongoDB, etc.\n",
    "* Ideal para pipelines de ingest√£o **bronze ‚Üí silver**.\n",
    "\n",
    "### ‚ö†Ô∏è Cuidado\n",
    "\n",
    "* Mais complexo que o `writeStream` padr√£o.\n",
    "* A fun√ß√£o `foreachBatch` roda em modo **batch dentro do streaming**, ent√£o **tem que ser eficiente**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resumo Comparativo\n",
    "\n",
    "| Crit√©rio               | `.writeStream` padr√£o   | `.foreachBatch`                            |\n",
    "| ---------------------- | ----------------------- | ------------------------------------------ |\n",
    "| L√≥gica por micro-batch | ‚ùå N√£o                   | ‚úÖ Sim                                      |\n",
    "| Suporte a UPSERT/MERGE | ‚ùå N√£o                   | ‚úÖ Sim (com Delta Lake)                     |\n",
    "| Destino suportado      | Limitado aos suportados | Qualquer destino (desde que c√≥digo exista) |\n",
    "| Complexidade           | üîπ Baixa                | üî∏ M√©dia/Alta                              |\n",
    "| Toler√¢ncia a falhas    | ‚úÖ Via checkpoint        | ‚úÖ Via checkpoint                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e78f267-61cd-42ec-9ac5-c261f416faf4",
   "metadata": {},
   "source": [
    "# C√≥digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c2aadc2-103d-49ca-bc51-364129cc5c58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_stream_json(landing_path, bronze_path, checkpoint_path, categoria, schema):\n",
    "    df_stream = spark.readStream \\\n",
    "        .format(\"json\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"multiline\",\"True\") \\\n",
    "        .load(f\"{landing_path}{categoria}\") \\\n",
    "        .withColumn(\"origem_arquivo\", input_file_name())\n",
    "\n",
    "    query = df_stream.writeStream \\\n",
    "        .format(\"json\") \\\n",
    "        .option(\"path\", f\"{bronze_path}{categoria}\") \\\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_path}{categoria}\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .start()\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f9e28c-1e68-41fe-9e9f-fe63318e3938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upsert_to_delta(micro_batch_df, batch_id, output_path):\n",
    "    if micro_batch_df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"id\").orderBy(F.col(\"timestamp\").desc())\n",
    "    deduplicated_df = micro_batch_df.withColumn(\"rn\", F.row_number().over(window_spec)) \\\n",
    "                                    .filter(F.col(\"rn\") == 1) \\\n",
    "                                    .drop(\"rn\")\n",
    "\n",
    "    if DeltaTable.isDeltaTable(spark, output_path):\n",
    "        delta_table = DeltaTable.forPath(spark, output_path)\n",
    "        delta_table.alias(\"t\").merge(\n",
    "            deduplicated_df.alias(\"s\"),\n",
    "            \"t.id = s.id\"\n",
    "        ).whenMatchedUpdateAll() \\\n",
    "         .whenNotMatchedInsertAll() \\\n",
    "         .execute()\n",
    "    else:\n",
    "        deduplicated_df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "\n",
    "    if DeltaTable.isDeltaTable(spark, output_path):\n",
    "        delta_table = DeltaTable.forPath(spark, output_path)\n",
    "        delta_table.alias(\"t\").merge(\n",
    "            micro_batch_df.alias(\"s\"),\n",
    "            \"t.id = s.id\"\n",
    "        ).whenMatchedUpdateAll() \\\n",
    "         .whenNotMatchedInsertAll() \\\n",
    "         .execute()\n",
    "    else:\n",
    "        micro_batch_df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "\n",
    "def process_stream(landing_path, bronze_path, checkpoint_path, categoria, schema):\n",
    "    input_path = f\"{landing_path}{categoria}\"\n",
    "    output_path = f\"{bronze_path}{categoria}\"\n",
    "    chk_path = f\"{checkpoint_path}{categoria}\"\n",
    "\n",
    "    df_stream = spark.readStream \\\n",
    "        .format(\"json\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"multiline\", \"True\") \\\n",
    "        .load(input_path) \\\n",
    "        .withColumn(\"origem_arquivo\", input_file_name())\n",
    "\n",
    "    query = df_stream.writeStream \\\n",
    "        .foreachBatch(lambda df, batch_id: upsert_to_delta(df, batch_id, output_path)) \\\n",
    "        .option(\"checkpointLocation\", chk_path) \\\n",
    "        .start()\n",
    "\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd2ef6-0ae1-4f7f-b45e-8e902ba08856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    process_stream(landing_path, bronze_path, checkpoint_path,\"usuarios\", schema_usuarios),\n",
    "    process_stream(landing_path, bronze_path, checkpoint_path,\"musicas\", schema_musicas),\n",
    "    process_stream(landing_path, bronze_path, checkpoint_path,\"streamings\", schema_streamings)\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3024beb-5912-41dd-af12-b4e7d266ae55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- email: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiline\",\"True\").json(f\"{landing_path}usuarios\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49ca3b55-dcc9-480b-ad41-01a03575c0bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Artist: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiline\",\"True\").json(f\"{landing_path}musicas\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e566a8e-4ed1-4eb4-86da-97e62614d710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- musica: string (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiline\",\"True\").json(f\"{landing_path}streamings\")\n",
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
